{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelExplainerクラス shapライブラリで分類結果を分析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import transformers\n",
    "\n",
    "class ModelExplainer():\n",
    "    def __init__(self, model, tokenizer, labels):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "    \n",
    "    def shap_explainer(self, features, answers, predicts):\n",
    "        pred = transformers.pipeline(\"text-classification\",model=self.model,tokenizer=self.tokenizer,device=0,return_all_scores=True)\n",
    "        explainer = shap.Explainer(pred,output_names=self.labels)\n",
    "        print(\"♦SHAP可視化結果\")\n",
    "        for feature, answer, predict in zip(features, answers, predicts):\n",
    "            print(f\"予測ラベル: {predict} ,正解ラベル: {answer}\")\n",
    "            print(feature)\n",
    "            shap_values = explainer([feature])\n",
    "            shap.plots.text(shap_values)\n",
    "            #shap.plots.text(shap_values[0,:,predict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excelシートの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "def makeClassificationResultSheet(classification_result, report_result, index2label, scores_df):\n",
    "    #予測ラベルから分布図を作成\n",
    "    predicted_label = classification_result['predicted_label']\n",
    "    count = collections.Counter(predicted_label)\n",
    "    print(count)\n",
    "    dist_label = []\n",
    "    for label in index2label.values():\n",
    "        if label in count:\n",
    "            dist_label.append(count[label])\n",
    "        else:\n",
    "            dist_label.append(0)\n",
    "    #グラフをメモリに一時保存\n",
    "    img = io.BytesIO()\n",
    "    fig = plt.figure(figsize=(4,3))\n",
    "    plt.bar(index2label.values(),dist_label)\n",
    "    fig.savefig(img,format='png')\n",
    "    #分類結果をxlsxで出力\n",
    "    writer = pd.ExcelWriter('classification_result.xlsx',engine='xlsxwriter')\n",
    "    classification_result.to_excel(writer,sheet_name='result',encoding='utf_8_sig',freeze_panes=[1,0])\n",
    "    #classification_reportを出力\n",
    "    report_result.to_excel(writer,sheet_name='classification_report')\n",
    "    #classification_scoresを出力\n",
    "    scores_df.to_excel(writer,sheet_name='classification_scores',freeze_panes=[1,0])\n",
    "    #エクセルシートの装飾\n",
    "    for column in classification_result:\n",
    "        column_length = max(classification_result[column].astype(str).map(len).max(),len(column))\n",
    "        col_idx = classification_result.columns.get_loc(column)\n",
    "        writer.sheets['result'].set_column(col_idx+1,col_idx+1,column_length)\n",
    "    \n",
    "    workbook = writer.book\n",
    "    color_format = workbook.add_format({'bg_color': '#9fff9c'})\n",
    "    row = len(classification_result.axes[0])+1\n",
    "    writer.sheets['result'].conditional_format('C2:C'+str(row),{\n",
    "        'type': 'formula',\n",
    "        'criteria': '=$B2=$C2',\n",
    "        'format': color_format\n",
    "    })\n",
    "    writer.sheets['result'].conditional_format('D2:D'+str(row),{\n",
    "        'type': 'formula',\n",
    "        'criteria': '=$B2=$D2',\n",
    "        'format': color_format\n",
    "    })\n",
    "    writer.sheets['result'].conditional_format('E2:E'+str(row),{\n",
    "        'type': 'formula',\n",
    "        'criteria': '=$B2=$E2',\n",
    "        'format': color_format\n",
    "    })\n",
    "    for index in range(len(scores_df.index)):\n",
    "        writer.sheets['classification_scores'].conditional_format(\n",
    "            'B'+str(index+2)+':'+'L'+str(index+2),\n",
    "            {'type': '3_color_scale',\n",
    "            'max_color': '#51f569',\n",
    "            'mid_color': 'white',\n",
    "            'min_color': '#f55151'}\n",
    "        )\n",
    "\n",
    "    writer.sheets['classification_report'].set_column(0,0,13)\n",
    "    #グラフを出力\n",
    "    writer.sheets['classification_report'].insert_image('G2','graph',{'image_data': img})\n",
    "    #上位3件正解率を計算\n",
    "    if len(index2label.values()) > 3:\n",
    "        cnt = 0\n",
    "        for index, row in classification_result.iterrows():\n",
    "            if row['answer_label'] == row['predicted_label']:\n",
    "                cnt += 1\n",
    "            elif row['answer_label'] == row['2nd_predicted']:\n",
    "                cnt += 1\n",
    "            elif row['answer_label'] == row['3rd_predicted']:\n",
    "                cnt += 1\n",
    "        top_3_accuracy_score = cnt/(len(classification_result.axes[0]))\n",
    "        writer.sheets['result'].write('A'+str(len(classification_result.axes[0])+3),'Top3:')\n",
    "        writer.sheets['result'].write('B'+str(len(classification_result.axes[0])+3),top_3_accuracy_score)\n",
    "    writer.save()\n",
    "\n",
    "def makeSecurityDetectorResultSheet(classification_result, report_result, index2label, scores_df):\n",
    "    #分類結果をxlsxで出力\n",
    "    writer = pd.ExcelWriter('classification_result_ml.xlsx',engine='xlsxwriter')\n",
    "    classification_result.to_excel(writer,sheet_name='result',encoding='utf_8_sig',freeze_panes=[1,0])\n",
    "    #classification_reportを出力\n",
    "    report_result.to_excel(writer,sheet_name='classification_report')\n",
    "    #classification_scoresを出力\n",
    "    scores_df.to_excel(writer,sheet_name='classification_scores',freeze_panes=[1,0])\n",
    "    #エクセルシートの装飾\n",
    "    for column in classification_result:\n",
    "        column_length = max(classification_result[column].astype(str).map(len).max(),len(column))\n",
    "        col_idx = classification_result.columns.get_loc(column)\n",
    "        writer.sheets['result'].set_column(col_idx+1,col_idx+1,column_length)\n",
    "    \n",
    "    workbook = writer.book\n",
    "    color_format = workbook.add_format({'bg_color': '#9fff9c'})\n",
    "    color_format2 = workbook.add_format({'bg_color': '#5465ff'})\n",
    "    row = len(classification_result.axes[0])+1\n",
    "    writer.sheets['result'].conditional_format('E2:E'+str(row),{\n",
    "        'type': 'formula',\n",
    "        'criteria': '=$C2=$E2',\n",
    "        'format': color_format2\n",
    "    })\n",
    "    writer.sheets['result'].conditional_format('D2:D'+str(row),{\n",
    "        'type': 'formula',\n",
    "        'criteria': '=$B2=$D2',\n",
    "        'format': color_format\n",
    "    })\n",
    "    for index in range(len(scores_df.index)):\n",
    "        writer.sheets['classification_scores'].conditional_format(\n",
    "            'B'+str(index+2)+':'+'L'+str(index+2),\n",
    "            {'type': '3_color_scale',\n",
    "            'max_color': '#51f569',\n",
    "            'mid_color': 'white',\n",
    "            'min_color': '#f55151'}\n",
    "        )\n",
    "\n",
    "    writer.sheets['classification_report'].set_column(0,0,13)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインポート、ファイルパスの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import StochasticWeightAveraging\n",
    "\n",
    "#MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-v2'\n",
    "\n",
    "TRAIN_PATH = 'datasets/cocoa/trainJP_nfr.txt'\n",
    "TEST_PATH = 'datasets/cocoa/testJP_nfr.txt'\n",
    "FOLD = 10\n",
    "PYTORCH_MODEL_DIR = './model_transformers/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Ligntning のクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier_pl(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, num_labels, lr, train_batch_size = 32):\n",
    "        #model_name: Transformersのモデル名\n",
    "        #num_labels: ラベルの数\n",
    "        #lr: 学習率\n",
    "        #train_batch_size: 学習データのバッチサイズ\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            attention_probs_dropout_prob=0.2,\n",
    "            hidden_dropout_prob=0.2,\n",
    "        )\n",
    "        \n",
    "    # 学習データのミニバッチの損失を出力\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    # 検証データのミニバッチの損失,精度を出力\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        labels = batch.pop('labels')\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        num_correct = (labels_predicted == labels).sum().item()\n",
    "        val_acc = num_correct/labels.size(0)\n",
    "        self.log('val_loss', val_loss)\n",
    "        self.log('val_acc',val_acc)\n",
    "        return {\"val_loss\": val_loss,\"val_acc\": val_acc}\n",
    "\n",
    "    # テストデータのミニバッチの精度を出力\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('labels')\n",
    "        output = self.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        num_correct = (labels_predicted == labels).sum().item()\n",
    "        accuracy = num_correct/labels.size(0)\n",
    "        self.log('accuracy', accuracy)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr,weight_decay=1e-2)\n",
    "        #optimizer = AdamW(self.parameters(),lr=self.hparams.lr,weight_decay=1e-2)\n",
    "        return [optimizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットを読み込んでBERTに入力可能な形式に変換する\n",
    "# dataset: 分割前データセット, dataset_idx: 分割前データセットのラベル\n",
    "def loadDatasets(train_path, test_path):\n",
    "    train_df = pd.read_csv(f'{train_path}', encoding='cp932')\n",
    "    test_df = pd.read_csv(f'{test_path}', encoding='cp932')\n",
    "\n",
    "    labels = {i: k for i, k in enumerate(train_df['label'].unique()) }#if k != 'PO'}\n",
    "    index2label = {i: k for i, k in enumerate(labels.values())}\n",
    "    label2index = {k: i for i, k in enumerate(labels.values())}\n",
    "\n",
    "    test_label_answer = test_df['label'].tolist()\n",
    "    test_data_list = test_df['feature'].tolist()\n",
    "    category_list = index2label.values()\n",
    "    num_labels = len(category_list)\n",
    "\n",
    "    dataset_for_loader = []\n",
    "    test_dataset_for_loader = []\n",
    "    category_num = []\n",
    "\n",
    "    max_length = 128  # トークン数\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    for label, category in enumerate(tqdm(category_list)):\n",
    "\n",
    "        tmp_train_df = train_df[train_df['label'] == category]\n",
    "        tmp_test_df = test_df[test_df['label'] == category]\n",
    "        category_num.append(len(tmp_train_df))\n",
    "        \n",
    "        for feature in tmp_train_df['feature']:\n",
    "            encoding = tokenizer(\n",
    "                feature,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True\n",
    "            )\n",
    "            encoding['labels'] = label  # ラベルを追加\n",
    "            encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "            dataset_for_loader.append(encoding)\n",
    "\n",
    "        for feature in tmp_test_df['feature']:\n",
    "            encoding = tokenizer(\n",
    "                feature,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True\n",
    "            )\n",
    "            encoding['labels'] = label  # ラベル(インデックス)を追加\n",
    "            encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "            test_dataset_for_loader.append(encoding)\n",
    "    \n",
    "    dataset_idx = [] #ラベル\n",
    "    cnt = 0\n",
    "    for num in category_num:\n",
    "        for i in range(num):\n",
    "            dataset_idx.append(cnt)\n",
    "        cnt += 1\n",
    "\n",
    "    return {\n",
    "        'num_labels': num_labels,\n",
    "        'index2label': index2label,\n",
    "        'label2index': label2index,\n",
    "        'dataset': dataset_for_loader,\n",
    "        'dataset_idx': dataset_idx,\n",
    "        'test_data': test_dataset_for_loader,\n",
    "        'test_data_answer': test_label_answer,\n",
    "        'test_data_list': test_data_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTをファインチューニングして保存する\n",
    "def trainingTaskKFold(data, generalization = False):\n",
    "    if os.path.exists('model/'):\n",
    "        shutil.rmtree('model/')\n",
    "    \n",
    "    if generalization is True:\n",
    "        num_of_training = FOLD\n",
    "    else:\n",
    "        num_of_training = 1\n",
    "    \n",
    "    dataloader_test = DataLoader(data['test_data'], batch_size=256)\n",
    "\n",
    "    #層化K分割で学習データと検証データに分割してファインチューニング\n",
    "    accuracy = []\n",
    "    best_model_paths = []\n",
    "    training_data_amounts = []\n",
    "    val_data_amounts = []\n",
    "    skf = StratifiedKFold(n_splits=FOLD)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=data['dataset'],y=data['dataset_idx'])):\n",
    "        \n",
    "        train_data = [data['dataset'][i] for i in train_index]\n",
    "        val_data = [data['dataset'][i] for i in val_index]\n",
    "        random.shuffle(train_data)\n",
    "        training_data_amounts.append(len(train_data))\n",
    "        val_data_amounts.append(len(val_data))\n",
    "        dataloader_train = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "        dataloader_val = DataLoader(val_data, batch_size=256)\n",
    "\n",
    "        model = BertClassifier_pl(\n",
    "            MODEL_NAME, num_labels=data['num_labels'], lr=1e-5,\n",
    "        )\n",
    "        checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "            filename=f'fold={fold+1}'+'-{epoch}-{step}-{val_loss:.1f}',\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_top_k=1,\n",
    "            #save_last=1,\n",
    "            save_weights_only=True,\n",
    "            dirpath='model/',\n",
    "        )\n",
    "        early_stop = (\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "                mode='min'\n",
    "            )\n",
    "        )\n",
    "        weight_averaging = (\n",
    "            StochasticWeightAveraging(swa_lrs=1e-5)\n",
    "        )\n",
    "        # 学習方法\n",
    "        trainer = pl.Trainer(\n",
    "            gpus=1,\n",
    "            max_epochs=40,\n",
    "            log_every_n_steps=10,\n",
    "            callbacks=[checkpoint, early_stop, weight_averaging]\n",
    "        )\n",
    "        # ファインチューニング\n",
    "        trainer.fit(model,train_dataloaders=dataloader_train,val_dataloaders=dataloader_val)\n",
    "\n",
    "        print('best model: ', checkpoint.best_model_path)\n",
    "        print('val_loss: ', checkpoint.best_model_score)\n",
    "        \n",
    "        best_model_path = checkpoint.best_model_path\n",
    "\n",
    "        test = trainer.test(dataloaders=dataloader_test,ckpt_path=best_model_path)\n",
    "        print(f'Accuracy: {test[0][\"accuracy\"]:.3f}')\n",
    "        accuracy.append(test[0][\"accuracy\"])\n",
    "\n",
    "        if num_of_training == 1:\n",
    "            model = BertClassifier_pl.load_from_checkpoint(best_model_path)\n",
    "            model.bert_sc.save_pretrained('./model_transformers/')\n",
    "            tmp_df = pd.DataFrame({\n",
    "                'amount': training_data_amounts,\n",
    "                'val_amounts': val_data_amounts\n",
    "            })\n",
    "            tmp_df.to_csv('./model/training_data_amounts.csv')\n",
    "            break\n",
    "        else:\n",
    "            best_model_paths.append(best_model_path)\n",
    "        \n",
    "    if num_of_training != 1:\n",
    "        print(f'Average accuracy: {np.mean(accuracy):.3f}')\n",
    "        print('Starting weight averaging task ...')\n",
    "        tmp_df = pd.DataFrame({\n",
    "            'amount': training_data_amounts,\n",
    "            'val_amount': val_data_amounts\n",
    "        })\n",
    "        tmp_df.to_csv('./model/training_data_amounts.csv')\n",
    "        #print(tmp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 予測、結果の出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存済みモデルで予測して評価する\n",
    "def predictAndEvaluate(data,mode='gpu'):\n",
    "    #保存済みモデルをロード\n",
    "    bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "        './model_transformers/'\n",
    "    )\n",
    "\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "    #符号化\n",
    "    encoding = tokenizer(\n",
    "        data['test_data_list'],\n",
    "        padding= 'longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    if mode == 'gpu':\n",
    "        #GPUにのせる\n",
    "        bert_sc = bert_sc.cuda()\n",
    "        encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "    else:\n",
    "        encoding = { k: v for k, v in encoding.items() }\n",
    "\n",
    "    #予測する\n",
    "    with torch.no_grad():\n",
    "        output = bert_sc.forward(**encoding)\n",
    "    scores = output.logits #分類スコア\n",
    "    print(scores)\n",
    "    labels_predicted = scores.argmax(-1) #スコアが最も高いラベルインデックス\n",
    "    labels_predicted_2nd = scores.argsort()[:,-2] #スコアが2番目に高いラベルインデックス\n",
    "    labels_predicted_3rd = scores.argsort()[:,-3] #スコアが3番目に高いラベルインデックス\n",
    "    #CPUに戻す\n",
    "    if mode == 'gpu':\n",
    "        labels_predicted = labels_predicted.cpu()\n",
    "        labels_predicted_2nd = labels_predicted_2nd.cpu()\n",
    "        labels_predicted_3rd = labels_predicted_3rd.cpu()\n",
    "        scores = scores.cpu()\n",
    "\n",
    "    #予測インデックスをラベルに変換する\n",
    "    predicted = []\n",
    "    predicted_2nd = []\n",
    "    predicted_3rd = []\n",
    "    for index in labels_predicted.tolist():\n",
    "        predicted.append(data['index2label'][index])\n",
    "    for index in labels_predicted_2nd.tolist():\n",
    "        predicted_2nd.append(data['index2label'][index])\n",
    "    for index in labels_predicted_3rd.tolist():\n",
    "        predicted_3rd.append(data['index2label'][index])\n",
    "\n",
    "    print('予測ラベル: ',predicted)\n",
    "    print('正解ラベル: ',data['test_data_answer'])\n",
    "\n",
    "    target_names = list(data['index2label'].values())\n",
    "    label_ids = list(data['index2label'].keys())\n",
    "\n",
    "    #正解ラベルをインデックスに変換する\n",
    "    ans_labels = []\n",
    "    for label in data['test_data_answer']:\n",
    "        ans_labels.append(data['label2index'][label])\n",
    "\n",
    "    report = classification_report(\n",
    "        ans_labels, labels_predicted, labels=label_ids, target_names=target_names, output_dict=True, zero_division=0\n",
    "    )\n",
    "\n",
    "    report_result = pd.DataFrame(report).T\n",
    "    display(report_result)\n",
    "\n",
    "    classification_result = pd.DataFrame({\n",
    "        'answer_label': data['test_data_answer'], \n",
    "        'predicted_label': predicted,\n",
    "        '2nd_predicted': predicted_2nd,\n",
    "        '3rd_predicted': predicted_3rd,\n",
    "        'text': data['test_data_list'],\n",
    "    },index=np.arange(1,len(predicted)+1))\n",
    "\n",
    "    scores_df = pd.DataFrame(scores,columns=data['index2label'].values(),index=np.arange(1,len(predicted)+1))\n",
    "\n",
    "    index2label = data['index2label']\n",
    "    makeClassificationResultSheet(classification_result,report_result,index2label,scores_df)\n",
    "\n",
    "    modelExplainer = ModelExplainer(model=bert_sc,tokenizer=tokenizer,labels=target_names)\n",
    "    modelExplainer.shap_explainer(data['test_data_list'],data['test_data_answer'],predicted)\n",
    "\n",
    "def predict(features):\n",
    "    #保存済みモデルをロード\n",
    "    bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "        './model_transformers/'\n",
    "    )\n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "    #符号化\n",
    "    encoding = tokenizer(\n",
    "        list(features),\n",
    "        padding= 'longest',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    bert_sc.cuda()\n",
    "    encoding = { k: v.cuda() for k, v in encoding.items() }\n",
    "    with torch.no_grad():\n",
    "        output = bert_sc.forward(**encoding)\n",
    "    return output.logits.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メインタスク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "print('Is CUDA available?: ',torch.cuda.is_available())\n",
    "\n",
    "data = loadDatasets(TRAIN_PATH, TEST_PATH)\n",
    "\n",
    "predictAndEvaluate(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e73d72f89a2fc200a9455b58ea7726ecb23ad963cee4e4b711ad60c0630b209d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
